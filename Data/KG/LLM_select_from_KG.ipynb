{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af61d6e3-27d5-4e4a-aab9-a3437ad28366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/shiyxu/myenv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/work/shiyxu/myenv/lib64/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# Import necessary functions\n",
    "from pyspark.sql.functions import count as spark_count, desc, col, split, broadcast, lit, rand, concat, floor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, when\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import KG_from_parquet as KGF\n",
    "from vllm import LLM, SamplingParams\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from graphframes import GraphFrame\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3025121-c6ea-4638-8f7e-e076e1f459d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/work/shiyxu/myenv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/shiyxu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/shiyxu/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-614bb946-abf0-4527-b094-99031905ab36;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 94ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-614bb946-abf0-4527-b094-99031905ab36\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "25/11/06 22:37:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 22:37:44 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 22:37:51 config.py:549] This model supports multiple tasks: {'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 11-06 22:37:51 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='../models/Llama-2-13B-chat-hf', speculative_config=None, tokenizer='../models/Llama-2-13B-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=../models/Llama-2-13B-chat-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 11-06 22:37:52 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 11-06 22:37:53 model_runner.py:1110] Starting to load model ../models/Llama-2-13B-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0841a1a83e4959a4f41016dff0afd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 22:38:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 22:38:07 model_runner.py:1115] Loading model weights took 24.2841 GB\n",
      "INFO 11-06 22:38:09 worker.py:267] Memory profiling takes 1.38 seconds\n",
      "INFO 11-06 22:38:09 worker.py:267] the current vLLM instance can use total_gpu_memory (47.40GiB) x gpu_memory_utilization (0.90) = 42.66GiB\n",
      "INFO 11-06 22:38:09 worker.py:267] model weights take 24.28GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 0.44GiB; the rest of the memory reserved for KV Cache is 17.88GiB.\n",
      "INFO 11-06 22:38:09 executor_base.py:111] # cuda blocks: 1464, # CPU blocks: 327\n",
      "INFO 11-06 22:38:09 executor_base.py:116] Maximum concurrency for 4096 tokens per request: 5.72x\n",
      "INFO 11-06 22:38:12 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:20<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-06 22:38:32 model_runner.py:1562] Graph capturing finished in 21 secs, took 0.24 GiB\n",
      "INFO 11-06 22:38:32 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 25.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting base edges...\n",
      "base_edges_df.count() 17549583\n",
      "Filtering edges by relation_set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 22:39:17 WARN TaskSetManager: Stage 9 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_edges 17549583\n",
      "Filtering edges by nodes_set (source)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 22:39:20 WARN TaskSetManager: Stage 13 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_edges 17549583\n",
      "Filtering edges by nodes_set (destination)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 22:39:25 WARN TaskSetManager: Stage 18 contains a task of very large size (6784 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_edges 13587287\n",
      "Creating Vertices DataFrame from nodes_set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 22:39:26 WARN TaskSetManager: Stage 19 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filteredv-_edges 2037048\n",
      "Creating GraphFrame object...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/shiyxu/myenv/lib64/python3.9/site-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rel_path = \"rebel_relations.pkl\"\n",
    "en_map = \"mapping.pkl\"\n",
    "entity_path = \"rebel_entities.pkl\"\n",
    "# spark initialization\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName(\"WikipediaProcessing-SkewDiagnosis\") # Changed AppName for clarity\n",
    "    .setMaster(\"local[10]\")  \n",
    "    .setAll(\n",
    "        [\n",
    "            (\"spark.driver.memory\", \"250g\"),\n",
    "            (\"spark.driver.maxResultSize\", \"32G\"), \n",
    "            (\"spark.memory.fraction\", \"0.75\"),\n",
    "            (\"spark.sql.shuffle.partitions\", \"2000\"), # 8000 may be excessive for just groupBy\n",
    "            (\"spark.driver.memoryOverhead\", \"8g\"),\n",
    "            (\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\")\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# initialize llm \n",
    "model_path = \"../models/Llama-2-13B-chat-hf\"\n",
    "llm = LLM(model_path)\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.2,  \n",
    "    max_tokens=50\n",
    ")\n",
    "# dics\n",
    "rel_df, mapping_df, entity_df = KGF.load_dictionaries(spark,rel_path,en_map,entity_path)\n",
    "# inputs\n",
    "all_rows = spark.read.parquet(\"./test_output/graph_data_all.parquet\")\n",
    "# graph\n",
    "graph_simple_gf = KGF.create_graphframe_from_spark(\n",
    "    spark, \n",
    "    all_rows, \n",
    "    mapping_df, \n",
    "    rel_df, \n",
    "    entity_df, \n",
    "    simple=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb1b4c4-a214-4da9-a315-0754e4e272e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/28 15:57:46 WARN TaskSetManager: Stage 22 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------------+\n",
      "|src|     dst|       relation_id|\n",
      "+---+--------+------------------+\n",
      "|Q31|   Q4916|     Q31_P38_Q4916|\n",
      "|Q31|Q1061257|Q31_P2852_Q1061257|\n",
      "|Q31| Q213107| Q31_P1313_Q213107|\n",
      "|Q31| Q128267|  Q31_P417_Q128267|\n",
      "|Q31|  Q41614|   Q31_P122_Q41614|\n",
      "|Q31| Q223933|  Q31_P793_Q223933|\n",
      "|Q31|Q1160895| Q31_P793_Q1160895|\n",
      "|Q31|     Q32|      Q31_P530_Q32|\n",
      "|Q31|     Q38|      Q31_P530_Q38|\n",
      "|Q31|    Q183|     Q31_P530_Q183|\n",
      "|Q31|    Q347|     Q31_P530_Q347|\n",
      "|Q31|    Q408|     Q31_P530_Q408|\n",
      "|Q31|    Q212|     Q31_P530_Q212|\n",
      "|Q31|   Q1246|    Q31_P530_Q1246|\n",
      "|Q31|    Q142|     Q31_P530_Q142|\n",
      "|Q31|    Q145|     Q31_P530_Q145|\n",
      "|Q31|     Q16|      Q31_P530_Q16|\n",
      "|Q31|    Q252|     Q31_P530_Q252|\n",
      "|Q31|     Q35|      Q31_P530_Q35|\n",
      "|Q31|     Q43|      Q31_P530_Q43|\n",
      "+---+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph_simple_gf.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "180c78c0-92d5-4dbc-a7c4-93d9a2022911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(graph_simple_gf,current_src ):\n",
    "    same_src_df = graph_simple_gf.edges.filter(F.col(\"src\") == current_src) # filter same src \n",
    "\n",
    "    # concat tables on selected id\n",
    "    edges_named = (\n",
    "    same_src_df\n",
    "    .withColumn(\"src_name\", lookup_name(\"src\"))\n",
    "    .withColumn(\"dst_name\", lookup_name(\"dst\"))\n",
    "    .withColumn(\"prop\", extract_property(\"relation_id\"))\n",
    "    .withColumn(\"prop_name\", lookup_name(\"prop\"))\n",
    "    .withColumn(\n",
    "        \"relation_name\",\n",
    "        F.concat_ws(\"\", F.concat(F.lit(\"<subj>\"), F.col(\"src_name\")), \n",
    "                    F.concat(F.lit(\"<rel>\"), F.col(\"prop_name\")),\n",
    "                    F.concat(F.lit(\"<obj>\"),F.col(\"dst_name\"))\n",
    "    )\n",
    "    )\n",
    "    )\n",
    "    start_name = edges_named.select(\"src_name\").first()[\"src_name\"]\n",
    "    edges_named.show()\n",
    "    relation_name_set = {row['relation_name'] for row in edges_named.select(\"relation_name\").collect()}\n",
    "    relations_dict = {\n",
    "    row['relation_id']: row['relation_name'] \n",
    "    for row in edges_named.select(\"relation_id\", \"relation_name\").collect()\n",
    "}\n",
    "    return relations_dict,edges_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e72fdf69-b111-4548-a404-a30d715ab5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 21:33:20 WARN TaskSetManager: Stage 260 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 21:33:20 WARN TaskSetManager: Stage 262 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "    relations_dict = {\n",
    "    row['relation_id']: row['relation_name'] \n",
    "    for row in edges_named.select(\"relation_id\", \"relation_name\").collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03d01392-2e25-41b9-be17-3ff9c8c76a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_dict =  {'Q8_P31_Q331769': '<subj>Happiness<rel>instance of<obj>Mood_(psychology)',\n",
    " 'Q8_P1343_Q1970746': '<subj>Happiness<rel>described by source<obj>Explanatory_Dictionary_of_the_Living_Great_Russian_Language',\n",
    " 'Q8_P1343_Q867541': '<subj>Happiness<rel>described by source<obj>Encyclopædia_Britannica_Eleventh_Edition',\n",
    " 'Q8_P460_Q203994': '<subj>Happiness<rel>said to be the same as<obj>Euphoria',\n",
    " 'Q8_P361_Q13100823': '<subj>Happiness<rel>part of<obj>Quality_of_life',\n",
    " 'Q8_P1552_Q487': '<subj>Happiness<rel>has quality<obj>Smile',\n",
    " 'Q8_P1552_Q1117179': '<subj>Happiness<rel>has quality<obj>Life_satisfaction'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf3801-a814-400a-a562-651831b58a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_triples=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "43e04051-c6c3-4101-a0d6-841e42d226ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.15s/it, est. speed input: 179.23 toks/s, output: 23.22 toks/s]\n"
     ]
    }
   ],
   "source": [
    "def choose_id_LLM(relations_dict, included_triples, llm, sampling_params):\n",
    "    prompt = f\"\"\"You are an AI evaluator. You have chose triples {included_triples} Select only one triple in triples dictionary that is most suitable to continue generating news. Don't repeat included triples. Output the keys as Qx_Px_Qx.\n",
    "relations:\n",
    "{relations_dict}\n",
    "output:\"\"\"\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    x = outputs[0].outputs[0].text.strip()\n",
    "    x=re.findall('Q\\d+\\_P\\d+\\_Q\\d+',x)\n",
    "    return x[0]\n",
    "    # ids = [x.strip() for x in x.split(',') if x.strip()]\n",
    "    # return ids[0]\n",
    "\n",
    "key=choose_id_LLM(relations_dict, included_triples, llm, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "850ec480-658a-4bff-9302-72a5a94ba90b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<subj>Happiness<rel>part of<obj>Quality_of_life'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c3e1f7b2-aff8-414e-b138-d933e94f1477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<subj>Happiness<rel>part of<obj>Quality_of_life',\n",
       " '<subj>Happiness<rel>part of<obj>Quality_of_life']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choose_id_LLM(start_node, relation_name_set, included_triples, llm, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "474aee16-3caf-4515-9511-32335345e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 166.15 toks/s, output: 22.22 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P361\\n\\nPlease select the most suitable triple to generate news for Happiness.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_node=\"Happiness\"\n",
    "included_triples=\"<subj>Happiness<rel>part of<obj>Quality_of_life\"\n",
    "relation_name_set= {'<id>P1343<name>described by source',\n",
    " '<id>P1552<name>has quality',\n",
    " '<id>P31<name>instance of',\n",
    " '<id>P361<name>part of',\n",
    " '<id>P460<name>said to be the same as'}\n",
    "prompt = f\"\"\"You are an AI evaluator. Select only one triple in triples that is most suitable to generate news for {start_node} based on {included_triples} Don't repeat same triples. Only output the relations id, formatted as pxx. Do NOT add extra information.\n",
    "relations:\n",
    "{relation_name_set}\n",
    "id:\"\"\"\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "x = outputs[0].outputs[0].text.strip()\n",
    "ids = [x.strip() for x in x.split(',') if x.strip()]\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bf630f8e-4bf7-44a1-bbd8-357bc1c604ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 22:06:41 WARN TaskSetManager: Stage 349 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 22:06:41 WARN TaskSetManager: Stage 350 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "same_src_df = graph_simple_gf.edges.filter(F.col(\"src\") == \"Q331769\") # filter same src = graph_simple_gf.edges.filter(F.col(\"src\") == current_src) # filter same src\n",
    "if same_src_df.isEmpty():\n",
    "    print(\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfc7d9-3aeb-45fd-9789-745799b300c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(en_map, \"rb\") as f:\n",
    "    id2name = pickle.load(f)\n",
    "id2name_broadcast = spark.sparkContext.broadcast(id2name)\n",
    "\n",
    "@F.udf(\"string\")\n",
    "def lookup_name(qid):\n",
    "    return id2name_broadcast.value.get(qid, qid)\n",
    "\n",
    "@F.udf(\"string\")\n",
    "def extract_property(rel_id):\n",
    "    parts = rel_id.split('_')\n",
    "    if len(parts) == 3:\n",
    "        return parts[1]  # e.g. Q31_P361_Q13116 → P361\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e22909-fee2-4d03-85f8-239e4c08d19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 23:41:44 WARN TaskSetManager: Stage 810 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:44 WARN TaskSetManager: Stage 811 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:45 WARN TaskSetManager: Stage 815 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:45 WARN TaskSetManager: Stage 816 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:47 WARN TaskSetManager: Stage 818 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:47 WARN TaskSetManager: Stage 820 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:49 WARN TaskSetManager: Stage 822 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "|src|     dst|       relation_id|src_name|            dst_name| prop|           prop_name|       relation_name|\n",
      "+---+--------+------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "|Q31|   Q4916|     Q31_P38_Q4916| Belgium|                Euro|  P38|            currency|<subj>Belgium<rel...|\n",
      "|Q31|Q1061257|Q31_P2852_Q1061257| Belgium|112_(emergency_te...|P2852|emergency phone n...|<subj>Belgium<rel...|\n",
      "|Q31| Q213107| Q31_P1313_Q213107| Belgium|Prime_Minister_of...|P1313|office held by he...|<subj>Belgium<rel...|\n",
      "|Q31| Q128267|  Q31_P417_Q128267| Belgium|        Saint_Joseph| P417|        patron saint|<subj>Belgium<rel...|\n",
      "|Q31|  Q41614|   Q31_P122_Q41614| Belgium|Constitutional_mo...| P122|basic form of gov...|<subj>Belgium<rel...|\n",
      "|Q31| Q223933|  Q31_P793_Q223933| Belgium|  Belgian_Revolution| P793|   significant event|<subj>Belgium<rel...|\n",
      "|Q31|Q1160895| Q31_P793_Q1160895| Belgium|Treaty_of_London_...| P793|   significant event|<subj>Belgium<rel...|\n",
      "|Q31|     Q32|      Q31_P530_Q32| Belgium|          Luxembourg| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|     Q38|      Q31_P530_Q38| Belgium|               Italy| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|    Q183|     Q31_P530_Q183| Belgium|             Germany| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|    Q347|     Q31_P530_Q347| Belgium|       Liechtenstein| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|    Q408|     Q31_P530_Q408| Belgium|           Australia| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|    Q212|     Q31_P530_Q212| Belgium|             Ukraine| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|   Q1246|    Q31_P530_Q1246| Belgium|              Kosovo| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|    Q142|     Q31_P530_Q142| Belgium|              France| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|    Q145|     Q31_P530_Q145| Belgium|      United_Kingdom| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|     Q16|      Q31_P530_Q16| Belgium|              Canada| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|    Q252|     Q31_P530_Q252| Belgium|           Indonesia| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|     Q35|      Q31_P530_Q35| Belgium|             Denmark| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "|Q31|     Q43|      Q31_P530_Q43| Belgium|              Turkey| P530| diplomatic relation|<subj>Belgium<rel...|\n",
      "+---+--------+------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 23:41:49 WARN TaskSetManager: Stage 824 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:50 WARN TaskSetManager: Stage 827 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:51 WARN TaskSetManager: Stage 828 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.28s/it, est. speed input: 346.95 toks/s, output: 21.90 toks/s]\n",
      "25/11/06 23:41:55 WARN TaskSetManager: Stage 830 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:55 WARN TaskSetManager: Stage 831 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionOutput(index=0, text='\\n                \\n                    Q31_P530_Q668\\n                    Q31_P530_Q183\\n                    Q31_P832_Q162', token_ids=(13, 462, 13, 462, 1678, 660, 29941, 29896, 29918, 29925, 29945, 29941, 29900, 29918, 29984, 29953, 29953, 29947, 13, 462, 1678, 660, 29941, 29896, 29918, 29925, 29945, 29941, 29900, 29918, 29984, 29896, 29947, 29941, 13, 462, 1678, 660, 29941, 29896, 29918, 29925, 29947, 29941, 29906, 29918, 29984, 29896, 29953, 29906), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)\n",
      "Q31_P530_Q668\n",
      "                    Q31_P530_Q183\n",
      "                    Q31_P832_Q162\n",
      "Q31_P530_Q668\n",
      "['Q31', 'Q31', 'Q668']\n",
      "['<subj>Belgium<rel>diplomatic relation<obj>India']\n",
      "Q668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 23:41:56 WARN TaskSetManager: Stage 835 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:56 WARN TaskSetManager: Stage 836 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:58 WARN TaskSetManager: Stage 838 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:58 WARN TaskSetManager: Stage 839 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:41:59 WARN TaskSetManager: Stage 842 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "| src|     dst|        relation_id|src_name|            dst_name| prop|           prop_name|       relation_name|\n",
      "+----+--------+-------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "|Q668|  Q80524|    Q668_P38_Q80524|   India|        Indian_rupee|  P38|            currency|<subj>India<rel>c...|\n",
      "|Q668|Q1061257|Q668_P2852_Q1061257|   India|112_(emergency_te...|P2852|emergency phone n...|<subj>India<rel>e...|\n",
      "|Q668|Q1267849| Q668_P172_Q1267849|   India|  Indo-Aryan_peoples| P172|        ethnic group|<subj>India<rel>e...|\n",
      "|Q668|  Q69798|   Q668_P172_Q69798|   India|   Dravidian_peoples| P172|        ethnic group|<subj>India<rel>e...|\n",
      "|Q668| Q192711| Q668_P1313_Q192711|   India|Prime_Minister_of...|P1313|office held by he...|<subj>India<rel>o...|\n",
      "|Q668| Q512187|  Q668_P122_Q512187|   India|    Federal_republic| P122|basic form of gov...|<subj>India<rel>b...|\n",
      "|Q668|Q1139536| Q668_P832_Q1139536|   India|Republic_Day_(India)| P832|      public holiday|<subj>India<rel>p...|\n",
      "|Q668|  Q47499|   Q668_P832_Q47499|   India|International_Wor...| P832|      public holiday|<subj>India<rel>p...|\n",
      "|Q668|  Q56106|   Q668_P832_Q56106|   India|Independence_Day_...| P832|      public holiday|<subj>India<rel>p...|\n",
      "|Q668| Q213380|  Q668_P209_Q213380|   India|Supreme_Court_of_...| P209|highest judicial ...|<subj>India<rel>h...|\n",
      "|Q668|  Q58705|   Q668_P206_Q58705|   India|         Arabian_Sea| P206|located in or nex...|<subj>India<rel>l...|\n",
      "|Q668|  Q38684|   Q668_P206_Q38684|   India|       Bay_of_Bengal| P206|located in or nex...|<subj>India<rel>l...|\n",
      "|Q668|   Q1239|    Q668_P206_Q1239|   India|        Indian_Ocean| P206|located in or nex...|<subj>India<rel>l...|\n",
      "|Q668|     Q38|      Q668_P530_Q38|   India|               Italy| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q155|     Q668_P530_Q155|   India|              Brazil| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q159|     Q668_P530_Q159|   India|              Russia| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q183|     Q668_P530_Q183|   India|             Germany| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q801|     Q668_P530_Q801|   India|              Israel| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|     Q30|      Q668_P530_Q30|   India|       United_States| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q408|     Q668_P530_Q408|   India|           Australia| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "+----+--------+-------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 23:42:00 WARN TaskSetManager: Stage 844 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:01 WARN TaskSetManager: Stage 847 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:02 WARN TaskSetManager: Stage 848 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.24s/it, est. speed input: 389.70 toks/s, output: 22.34 toks/s]\n",
      "25/11/06 23:42:05 WARN TaskSetManager: Stage 850 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionOutput(index=0, text='\\n                \\n                    Q668_P530_Q142\\n                    Q668_P530_Q858\\n                    Q668_P1365_', token_ids=(13, 462, 13, 462, 1678, 660, 29953, 29953, 29947, 29918, 29925, 29945, 29941, 29900, 29918, 29984, 29896, 29946, 29906, 13, 462, 1678, 660, 29953, 29953, 29947, 29918, 29925, 29945, 29941, 29900, 29918, 29984, 29947, 29945, 29947, 13, 462, 1678, 660, 29953, 29953, 29947, 29918, 29925, 29896, 29941, 29953, 29945, 29918), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)\n",
      "Q668_P530_Q142\n",
      "                    Q668_P530_Q858\n",
      "                    Q668_P1365_\n",
      "Q668_P530_Q142\n",
      "['Q31', 'Q31', 'Q668', 'Q668', 'Q142']\n",
      "['<subj>Belgium<rel>diplomatic relation<obj>India', '<subj>India<rel>diplomatic relation<obj>France']\n",
      "Q668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 23:42:06 WARN TaskSetManager: Stage 852 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:07 WARN TaskSetManager: Stage 854 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:07 WARN TaskSetManager: Stage 856 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:08 WARN TaskSetManager: Stage 859 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:09 WARN TaskSetManager: Stage 860 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:10 WARN TaskSetManager: Stage 863 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "| src|     dst|        relation_id|src_name|            dst_name| prop|           prop_name|       relation_name|\n",
      "+----+--------+-------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "|Q668|  Q80524|    Q668_P38_Q80524|   India|        Indian_rupee|  P38|            currency|<subj>India<rel>c...|\n",
      "|Q668|Q1061257|Q668_P2852_Q1061257|   India|112_(emergency_te...|P2852|emergency phone n...|<subj>India<rel>e...|\n",
      "|Q668|Q1267849| Q668_P172_Q1267849|   India|  Indo-Aryan_peoples| P172|        ethnic group|<subj>India<rel>e...|\n",
      "|Q668|  Q69798|   Q668_P172_Q69798|   India|   Dravidian_peoples| P172|        ethnic group|<subj>India<rel>e...|\n",
      "|Q668| Q192711| Q668_P1313_Q192711|   India|Prime_Minister_of...|P1313|office held by he...|<subj>India<rel>o...|\n",
      "|Q668| Q512187|  Q668_P122_Q512187|   India|    Federal_republic| P122|basic form of gov...|<subj>India<rel>b...|\n",
      "|Q668|Q1139536| Q668_P832_Q1139536|   India|Republic_Day_(India)| P832|      public holiday|<subj>India<rel>p...|\n",
      "|Q668|  Q47499|   Q668_P832_Q47499|   India|International_Wor...| P832|      public holiday|<subj>India<rel>p...|\n",
      "|Q668|  Q56106|   Q668_P832_Q56106|   India|Independence_Day_...| P832|      public holiday|<subj>India<rel>p...|\n",
      "|Q668| Q213380|  Q668_P209_Q213380|   India|Supreme_Court_of_...| P209|highest judicial ...|<subj>India<rel>h...|\n",
      "|Q668|  Q58705|   Q668_P206_Q58705|   India|         Arabian_Sea| P206|located in or nex...|<subj>India<rel>l...|\n",
      "|Q668|  Q38684|   Q668_P206_Q38684|   India|       Bay_of_Bengal| P206|located in or nex...|<subj>India<rel>l...|\n",
      "|Q668|   Q1239|    Q668_P206_Q1239|   India|        Indian_Ocean| P206|located in or nex...|<subj>India<rel>l...|\n",
      "|Q668|     Q38|      Q668_P530_Q38|   India|               Italy| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q155|     Q668_P530_Q155|   India|              Brazil| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q159|     Q668_P530_Q159|   India|              Russia| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q183|     Q668_P530_Q183|   India|             Germany| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q801|     Q668_P530_Q801|   India|              Israel| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|     Q30|      Q668_P530_Q30|   India|       United_States| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "|Q668|    Q408|     Q668_P530_Q408|   India|           Australia| P530| diplomatic relation|<subj>India<rel>d...|\n",
      "+----+--------+-------------------+--------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 23:42:11 WARN TaskSetManager: Stage 864 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:12 WARN TaskSetManager: Stage 866 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 23:42:12 WARN TaskSetManager: Stage 867 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.23s/it, est. speed input: 401.74 toks/s, output: 22.44 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionOutput(index=0, text='\\n                \\n                    Q668_P530_Q774\\n                    Q668_P530_Q884\\n                    Q668_P2936_', token_ids=(13, 462, 13, 462, 1678, 660, 29953, 29953, 29947, 29918, 29925, 29945, 29941, 29900, 29918, 29984, 29955, 29955, 29946, 13, 462, 1678, 660, 29953, 29953, 29947, 29918, 29925, 29945, 29941, 29900, 29918, 29984, 29947, 29947, 29946, 13, 462, 1678, 660, 29953, 29953, 29947, 29918, 29925, 29906, 29929, 29941, 29953, 29918), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)\n",
      "Q668_P530_Q774\n",
      "                    Q668_P530_Q884\n",
      "                    Q668_P2936_\n",
      "Q668_P530_Q774\n",
      "['Q31', 'Q31', 'Q668', 'Q668', 'Q142', 'Q668', 'Q774']\n",
      "['<subj>Belgium<rel>diplomatic relation<obj>India', '<subj>India<rel>diplomatic relation<obj>France', '<subj>India<rel>diplomatic relation<obj>Guatemala']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def filter_df(graph_simple_gf,current_src ):\n",
    "    same_src_df = graph_simple_gf.edges.filter(F.col(\"src\") == current_src) # filter same src \n",
    "    if same_src_df.isEmpty():\n",
    "        return None, None\n",
    "    else:\n",
    "        # concat tables on selected id\n",
    "        edges_named = (\n",
    "        same_src_df\n",
    "        .withColumn(\"src_name\", lookup_name(\"src\"))\n",
    "        .withColumn(\"dst_name\", lookup_name(\"dst\"))\n",
    "        .withColumn(\"prop\", extract_property(\"relation_id\"))\n",
    "        .withColumn(\"prop_name\", lookup_name(\"prop\"))\n",
    "        .withColumn(\n",
    "            \"relation_name\",\n",
    "            F.concat_ws(\"\", F.concat(F.lit(\"<subj>\"), F.col(\"src_name\")), \n",
    "                        F.concat(F.lit(\"<rel>\"), F.col(\"prop_name\")),\n",
    "                        F.concat(F.lit(\"<obj>\"),F.col(\"dst_name\"))\n",
    "        )\n",
    "        )\n",
    "        )\n",
    "        start_name = edges_named.select(\"src_name\").first()[\"src_name\"]\n",
    "        edges_named.show()\n",
    "        relation_name_set = {row['relation_name'] for row in edges_named.select(\"relation_name\").collect()}\n",
    "        \n",
    "        relations_dict = {\n",
    "        row['relation_id']: row['relation_name'] \n",
    "        for row in edges_named.select(\"relation_id\", \"relation_name\").collect()\n",
    "    }\n",
    "        return relations_dict,edges_named\n",
    "def choose_id_LLM(relations_dict, included_triples, llm, sampling_params):\n",
    "    prompt = f\"\"\"You are an AI evaluator.  Select triples in triples dictionary that are suitable to continue generating news based on {included_triples}. Output the keys as Qx_Px_Qx.\n",
    "                \n",
    "                relations:\n",
    "                {relations_dict}\n",
    "                \n",
    "                output:\"\"\"\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    print(outputs[0].outputs[0])\n",
    "    x = outputs[0].outputs[0].text.strip()\n",
    "    print(x)\n",
    "    x=re.findall('Q\\d+\\_P\\d+\\_Q\\d+',x)\n",
    "    return x[0]\n",
    "    # ids = [x.strip() for x in x.split(',') if x.strip()]\n",
    "    # return ids[0]\n",
    "\n",
    "import random\n",
    "for start_node in[\"Q31\"]:\n",
    "    num_triplets = 4\n",
    "    entities_in_set = [start_node]\n",
    "    included_triples=[]\n",
    "    for r in range(2, num_triplets+1):\n",
    "        current_src = random.choice(list(set(entities_in_set)))\n",
    "        print(current_src)\n",
    "        relations_dict,edges_named=filter_df(graph_simple_gf,current_src)\n",
    "        \n",
    "        if len(relations_dict)>20:\n",
    "            all_keys = list(relations_dict.keys())\n",
    "            random_keys = random.sample(all_keys, 20)\n",
    "            relations_dict = {key: relations_dict[key] for key in random_keys}\n",
    "        relations_dict.pop(next_triple_ids,None)\n",
    "        if relations_dict is not None: \n",
    "            try:\n",
    "                next_triple_ids = choose_id_LLM(relations_dict, included_triples, llm, sampling_params)\n",
    "                print(next_triple_ids)\n",
    "                # next_rel_id = re.findall(\"P\\d+\", next_rel_id)[0]\n",
    "                # same_src_rel_def= edges_named.filter(F.lower(F.col(\"prop\"))==next_rel_id.lower())\n",
    "                # rows_list = same_src_rel_def.select(\"relation_name\").collect()\n",
    "                # objs=same_src_rel_def.select(\"dst\").collect()\n",
    "                # string_list = [row['relation_name'] for row in rows_list]\n",
    "                # objs= [row['dst'] for row in objs]\n",
    "                # included_triples.extend(string_list)\n",
    "                # entities_in_set.extend(objs)\n",
    "                # print(included_triples)\n",
    "                # print(entities_in_set)\n",
    "                entities_in_set.extend(re.findall(\"Q\\d+\", next_triple_ids))\n",
    "                included_triples.append(relations_dict[next_triple_ids])\n",
    "                print(entities_in_set)\n",
    "                print(included_triples)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            included_triples.append(\"none\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7e568c3-4fa4-46ef-9aea-d597a04c897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_in_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76a9bcdf-6683-4e60-83f7-31b2dbcc94de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q8', 'Q331769']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"Q\\d+\", next_triple_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6dbf5b84-72e1-434d-9328-beb1b43384d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "list2=['Q8', 'Q331769']\n",
    "list1=['Q8']\n",
    "list1=list1.extend(list2)\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "808768c1-62f6-452d-af36-f33e03c4bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 20:03:34 WARN TaskSetManager: Stage 118 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 20:03:34 WARN TaskSetManager: Stage 120 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 20:03:35 WARN TaskSetManager: Stage 122 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 20:03:35 WARN TaskSetManager: Stage 124 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+---------------+------------------+----+-----------+--------------------+\n",
      "|      src|    dst|         relation_id|       src_name|          dst_name|prop|  prop_name|       relation_name|\n",
      "+---------+-------+--------------------+---------------+------------------+----+-----------+--------------------+\n",
      "|Q13100823|Q185957|Q13100823_P279_Q1...|Quality_of_life|Quality_(business)|P279|subclass of|<subj>Quality_of_...|\n",
      "|Q13100823|Q853725|Q13100823_P279_Q8...|Quality_of_life|   Social_relation|P279|subclass of|<subj>Quality_of_...|\n",
      "|Q13100823|Q151885|Q13100823_P31_Q15...|Quality_of_life|           Concept| P31|instance of|<subj>Quality_of_...|\n",
      "+---------+-------+--------------------+---------------+------------------+----+-----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 20:03:37 WARN TaskSetManager: Stage 128 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 20:03:37 WARN TaskSetManager: Stage 129 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Quality_of_life', {'<id>P279<name>subclass of', '<id>P31<name>instance of'})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_df(graph_simple_gf,current_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d927243a-4d23-4eda-ad7a-ce6797915997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P31\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "next_rel_id\n",
    "print(re.findall(\"P\\d+\", next_rel_id)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65c1dc5f-5e13-43bd-95a5-557c75d14af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 19:44:00 WARN TaskSetManager: Stage 50 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:44:01 WARN TaskSetManager: Stage 52 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:44:02 WARN TaskSetManager: Stage 54 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:44:03 WARN TaskSetManager: Stage 55 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "same_src_rel_def= edges_named.filter(F.lower(F.col(\"prop\"))==next_rel_id.lower())\n",
    "rows_list = same_src_rel_def.select(\"relation_name\").collect()\n",
    "objs=same_src_rel_def.select(\"dst\").collect()\n",
    "string_list = [row['relation_name'] for row in rows_list]\n",
    "objs= [row['dst'] for row in objs]\n",
    "included_triples.extend(string_list)\n",
    "entities_in_set.extend(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "133ad0a4-ca1b-447a-a046-b1b19b454b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q13100823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 19:44:39 WARN TaskSetManager: Stage 59 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:44:39 WARN TaskSetManager: Stage 60 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:44:40 WARN TaskSetManager: Stage 62 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:44:41 WARN TaskSetManager: Stage 64 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "current_src = random.choice(list(entities_in_set))\n",
    "print(current_src)\n",
    "prop_pairs_set=filter_df(graph_simple_gf,current_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ed77ef-37fc-433d-81fd-b6c604be9564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 19:05:48 WARN TaskSetManager: Stage 23 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:05:49 WARN TaskSetManager: Stage 24 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:05:51 WARN TaskSetManager: Stage 26 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:05:51 WARN TaskSetManager: Stage 27 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "current_src=\"Q8\"\n",
    "same_src_df = graph_simple_gf.edges.filter(F.col(\"src\") == current_src) # filter same src \n",
    "\n",
    "# concat tables on selected id\n",
    "edges_named = (\n",
    "same_src_df\n",
    ".withColumn(\"src_name\", lookup_name(\"src\"))\n",
    ".withColumn(\"dst_name\", lookup_name(\"dst\"))\n",
    ".withColumn(\"prop\", extract_property(\"relation_id\"))\n",
    ".withColumn(\"prop_name\", lookup_name(\"prop\"))\n",
    ".withColumn(\n",
    "    \"relation_name\",\n",
    "    F.concat_ws(\"\", F.concat(F.lit(\"<subj>\"), F.col(\"src_name\")), \n",
    "                F.concat(F.lit(\"<rel>\"), F.col(\"prop_name\")),\n",
    "                F.concat(F.lit(\"<obj>\"),F.col(\"dst_name\"))\n",
    ")\n",
    ")\n",
    ")\n",
    "start_name = edges_named.select(\"src_name\").first()[\"src_name\"]\n",
    "\n",
    "\n",
    "# 2️⃣ 收集 relation_name 列\n",
    "prop_pairs = edges_named.select(\"prop\", \"prop_name\") \\\n",
    "    .rdd.map(lambda row: f\"<id>{row['prop']}<name>{row['prop_name']}\") \\\n",
    "    .collect()\n",
    "prop_pairs_set = set(prop_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2854257b-232f-45be-bc95-c75ef0846872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_id_LLM(src, prop_pairs_set, included_triples, llm, sampling_params):\n",
    "    prompt = f\"\"\"You are an AI evaluator. Select only one relation in relations that is most suitable to generate news for {src}, with the included triples. Don't repeat same choices in included triples. Only output the relations id, formatted as pxx. Do NOT add extra information.\n",
    "included triples:{included_triples}\n",
    "Relations:\n",
    "{prop_pairs_set}\n",
    "id:\"\"\"\n",
    "    outputs = llm.generate([prompt], sampling_params)\n",
    "    x = outputs[0].outputs[0].text.strip()\n",
    "    ids = [x.strip() for x in x.split(',') if x.strip()]\n",
    "    return ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74604b21-830b-4c06-a770-bdcef2a3bbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src=current_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c198f26f-e628-46b1-b27d-2f06684f7f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<id>P1343<name>described by source',\n",
       " '<id>P1552<name>has quality',\n",
       " '<id>P31<name>instance of',\n",
       " '<id>P361<name>part of',\n",
       " '<id>P460<name>said to be the same as'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_pairs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e00fd54-03b1-4424-9ffb-476ab6478ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_triples=string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fa53e49-e39f-4821-99d9-b1a5538715d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.10it/s, est. speed input: 155.66 toks/s, output: 22.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P361\n",
      "\n",
      "Please select the most suitable relation to generate news for Q8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(choose_id_LLM(src, prop_pairs_set,included_triples, llm, sampling_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0a5585b-04f3-4d2e-9903-cada4704cef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/06 19:24:32 WARN TaskSetManager: Stage 31 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/06 19:24:33 WARN TaskSetManager: Stage 32 contains a task of very large size (2608 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "same_src_rel_def= edges_named.filter(F.lower(F.col(\"prop\"))==\"p361\")\n",
    "rows_list = same_src_rel_def.select(\"relation_name\").collect()\n",
    "string_list = [row['relation_name'] for row in rows_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6337ea8-da0d-4ae0-ac38-dde0d513891b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<subj>Happiness<rel>part of<obj>Quality_of_life']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
