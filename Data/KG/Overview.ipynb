{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcc1f23-8388-4f56-9f49-d534518d408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q3285447', 'Q42317667', 'Q7068799', 'Q5443871', 'Q7918771']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_pickle(\"rebel_entities.pkl\")\n",
    "print(list(df)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec8d487-6c43-4ab8-a0e9-3c7286505a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P103', 'P1571', 'P485', 'P571', 'P6438']\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_pickle(\"rebel_relations.pkl\")\n",
    "print(list(df)[:5]\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "802670d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmapping.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "df=pd.read_pickle(\"mapping.pkl\")\n",
    "print(df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703245d6-8f4b-4ca6-8222-09b35196c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import pow\n",
    "import networkx as nx  # Import NetworkX\n",
    "\n",
    "# --- Load NetworkX graph from 'final_graph.pkl' ---\n",
    "try:\n",
    "    with open('final_graph.pkl', 'rb') as file:\n",
    "        # G is now a NetworkX Graph object\n",
    "        G = pickle.load(file)\n",
    "    \n",
    "    if not isinstance(G, nx.Graph):\n",
    "        print(\"Warning: The loaded object may not be a NetworkX graph.\")\n",
    "\n",
    "    print(f\"Successfully loaded NetworkX graph from 'final_graph.pkl'.\")\n",
    "    print(f\"The graph contains {G.number_of_nodes():,} nodes and {G.number_of_edges():,} edges.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_graph.pkl' file not found.\")\n",
    "    print(\"Please ensure the file is in the same directory as your Python script.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or processing the file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Extract triples ---\n",
    "\n",
    "RELATION_ATTRIBUTE_KEY = 'relation'\n",
    "\n",
    "print(f\"Extracting triples from the NetworkX graph (relation attribute key: '{RELATION_ATTRIBUTE_KEY}')...\")\n",
    "KG_triplets = []  # List to store all triples\n",
    "KG_dict = {}      # Dictionary for fast lookup\n",
    "\n",
    "# Iterate over each edge in the graph to construct triples\n",
    "# G.edges(data=True) returns (source node, target node, attribute dictionary)\n",
    "for u, v, data in G.edges(data=True):\n",
    "    # Retrieve the relation from the attribute dictionary, if it exists\n",
    "    relation = data.get(RELATION_ATTRIBUTE_KEY)\n",
    "    if relation:\n",
    "        # Create a triple (subject, predicate, object)\n",
    "        triplet = (u, relation, v)\n",
    "        KG_triplets.append(triplet)\n",
    "        \n",
    "        # Populate KG_dict for fast lookup\n",
    "        s, p, o = triplet\n",
    "        if s not in KG_dict:\n",
    "            KG_dict[s] = []\n",
    "        if o not in KG_dict:\n",
    "            KG_dict[o] = []\n",
    "        KG_dict[s].append(triplet)\n",
    "        KG_dict[o].append(triplet)\n",
    "\n",
    "if not KG_triplets:\n",
    "    print(\"Error: Failed to extract any triples from the graph.\")\n",
    "    print(f\"Please check whether the edges in your NetworkX graph contain the attribute '{RELATION_ATTRIBUTE_KEY}'.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Extraction complete. Obtained {len(KG_triplets):,} valid triples.\")\n",
    "\n",
    "# --- Step 1: Function to select a “starting” triplet (no modification required) ---\n",
    "def get_start_triplet(kg_triplets_list, dampening_factor=0.01):\n",
    "    relations = [p for _, p, _ in kg_triplets_list]\n",
    "    if not relations:\n",
    "        raise ValueError(\"The knowledge graph is empty or contains no relations.\")\n",
    "    \n",
    "    relation_counts = Counter(relations)\n",
    "    relations_unique = list(relation_counts.keys())\n",
    "    weights = [pow(relation_counts[p], dampening_factor) for p in relations_unique]\n",
    "    chosen_relation = random.choices(relations_unique, weights=weights, k=1)[0]\n",
    "    possible_triplets = [triplet for triplet in kg_triplets_list if triplet[1] == chosen_relation]\n",
    "    \n",
    "    return random.choice(possible_triplets)\n",
    "\n",
    "# --- Step 2: Function to build a “coherent triplet set” (no modification required) ---\n",
    "def sample_coherent_triplet_set(kg_lookup_dict, start_triplet, bias_factor=7.0, mean_size=3.0):\n",
    "    num_triplets = np.random.poisson(lam=mean_size)\n",
    "    if num_triplets == 0:\n",
    "        return []\n",
    "\n",
    "    triplet_set = [start_triplet]\n",
    "    entities_in_set = {start_triplet[0], start_triplet[2]}\n",
    "\n",
    "    for r in range(2, num_triplets + 1):\n",
    "        if not entities_in_set:\n",
    "            break\n",
    "        anchor_entity = random.choice(list(entities_in_set))\n",
    "        \n",
    "        candidate_triplets = [\n",
    "            triplet for triplet in kg_lookup_dict.get(anchor_entity, [])\n",
    "            if triplet not in triplet_set\n",
    "        ]\n",
    "\n",
    "        if not candidate_triplets:\n",
    "            continue\n",
    "\n",
    "        weights = []\n",
    "        N = len(entities_in_set)\n",
    "        \n",
    "        for triplet in candidate_triplets:\n",
    "            other_entity = triplet[0] if triplet[2] == anchor_entity else triplet[2]\n",
    "            \n",
    "            if other_entity in entities_in_set:\n",
    "                base = N + 1 - r\n",
    "                weight = pow(base, bias_factor) if base > 0 else 0\n",
    "            else:\n",
    "                weight = 1.0\n",
    "            weights.append(weight)\n",
    "\n",
    "        if sum(weights) == 0:\n",
    "            continue\n",
    "        \n",
    "        chosen_triplet = random.choices(candidate_triplets, weights=weights, k=1)[0]\n",
    "        \n",
    "        triplet_set.append(chosen_triplet)\n",
    "        entities_in_set.add(chosen_triplet[0])\n",
    "        entities_in_set.add(chosen_triplet[2])\n",
    "        \n",
    "    return triplet_set\n",
    "\n",
    "# --- Main program: Run sampling using data converted from the NetworkX graph ---\n",
    "if __name__ == \"__main__\":\n",
    "    BIAS_FACTOR = 7.0\n",
    "    DAMPENING_FACTOR = 0.01\n",
    "    MEAN_TRIPLET_SET_SIZE = 3.0\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 1. Select a starting triplet from the extracted triplet list\n",
    "    print(\"Sampling starting triplet...\")\n",
    "    start_triplet = get_start_triplet(KG_triplets, dampening_factor=DAMPENING_FACTOR)\n",
    "    print(f\"Randomly selected starting triplet: {start_triplet}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 2. Build a coherent triplet set based on the starting point and lookup dictionary\n",
    "    print(\"Generating coherent triplet set...\")\n",
    "    coherent_set = sample_coherent_triplet_set(\n",
    "        kg_lookup_dict=KG_dict,\n",
    "        start_triplet=start_triplet,\n",
    "        bias_factor=BIAS_FACTOR,\n",
    "        mean_size=MEAN_TRIPLET_SET_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"\\nGenerated coherent triplet set of size {len(coherent_set)}:\")\n",
    "    if not coherent_set:\n",
    "        print(\"  (The sampled set is empty this time.)\")\n",
    "    else:\n",
    "        for triplet in coherent_set:\n",
    "            print(f\"  {triplet}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
